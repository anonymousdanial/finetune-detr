{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e74191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1a4589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main\n",
      "/mnt/d/23b6034/FYP/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/d/23b6034/FYP/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Set TORCH_HOME to your desired location\n",
    "os.environ['TORCH_HOME'] = r'/mnt/d/23b6034/FYP/models'\n",
    "\n",
    "# Now load the model\n",
    "model2 = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9966d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attrs: ['T_destination', 'add_module', 'apply', 'aux_loss', 'backbone', 'bbox_embed', 'bfloat16', 'buffers', 'call_super_init', 'children', 'class_embed', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'input_proj', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_queries', 'parameters', 'query_embed', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'transformer', 'type', 'xpu', 'zero_grad']\n",
      "Saved parts to /mnt/d/23b6034/FYP/models/detr_parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main\n",
      "/mnt/d/23b6034/FYP/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/d/23b6034/FYP/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parts into new_model (strict=False)\n"
     ]
    }
   ],
   "source": [
    "# Save individual parts of the DETR model to separate files\n",
    "\n",
    "# Helper to create simple filenames\n",
    "model_dir = os.path.join(os.environ.get('TORCH_HOME', '.'), 'detr_parts')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "paths = {\n",
    "    'backbone': os.path.join(model_dir, 'detr_backbone.pth'),\n",
    "    'transformer_encoder': os.path.join(model_dir, 'detr_transformer_encoder.pth'),\n",
    "    'transformer_decoder': os.path.join(model_dir, 'detr_transformer_decoder.pth'),\n",
    "    'head': os.path.join(model_dir, 'detr_head.pth'),\n",
    "}\n",
    "\n",
    "# Inspecting model structure (adapt these attribute names if your DETR variant differs)\n",
    "# For the facebookresearch DETR, typical attributes are: backbone, transformer, and class/box heads under model or a separate DETR wrapper.\n",
    "print('Model attrs:', [a for a in dir(model2) if not a.startswith('_')])\n",
    "\n",
    "# Extract state_dicts for each part (best-effort; adjust keys according to your model)\n",
    "full_sd = model2.state_dict()\n",
    "\n",
    "backbone_sd = {k.replace('backbone.', ''): v for k, v in full_sd.items() if k.startswith('backbone.')}\n",
    "transformer_encoder_sd = {k.replace('transformer.encoder.', ''): v for k, v in full_sd.items() if k.startswith('transformer.encoder.')}\n",
    "transformer_decoder_sd = {k.replace('transformer.decoder.', ''): v for k, v in full_sd.items() if k.startswith('transformer.decoder.')}\n",
    "# Heads often live under 'class_embed' / 'bbox_embed' or 'heads' depending on implementation\n",
    "head_keys = [k for k in full_sd.keys() if ('class_embed' in k) or ('bbox_embed' in k) or k.startswith('bbox_head') or 'head' in k]\n",
    "head_sd = {k: full_sd[k] for k in head_keys}\n",
    "\n",
    "# Save parts\n",
    "torch.save(backbone_sd, paths['backbone'])\n",
    "torch.save(transformer_encoder_sd, paths['transformer_encoder'])\n",
    "torch.save(transformer_decoder_sd, paths['transformer_decoder'])\n",
    "torch.save(head_sd, paths['head'])\n",
    "\n",
    "print('Saved parts to', model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05be1482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using builder: models.detr.build\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'position_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Build model using the args namespace\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     new_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Some builders return (model, ...) -> handle both cases\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     out \u001b[38;5;241m=\u001b[39m builder(args)\n",
      "File \u001b[0;32m/mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main/models/detr.py:320\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    317\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[1;32m    318\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 320\u001b[0m backbone \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transformer \u001b[38;5;241m=\u001b[39m build_transformer(args)\n\u001b[1;32m    324\u001b[0m model \u001b[38;5;241m=\u001b[39m DETR(\n\u001b[1;32m    325\u001b[0m     backbone,\n\u001b[1;32m    326\u001b[0m     transformer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     aux_loss\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39maux_loss,\n\u001b[1;32m    330\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main/models/backbone.py:113\u001b[0m, in \u001b[0;36mbuild_backbone\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_backbone\u001b[39m(args):\n\u001b[0;32m--> 113\u001b[0m     position_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_position_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     train_backbone \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mlr_backbone \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    115\u001b[0m     return_interm_layers \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mmasks\n",
      "File \u001b[0;32m/mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main/models/position_encoding.py:81\u001b[0m, in \u001b[0;36mbuild_position_encoding\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_position_encoding\u001b[39m(args):\n\u001b[1;32m     80\u001b[0m     N_steps \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mhidden_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msine\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# TODO find a better way of exposing other arguments\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         position_embedding \u001b[38;5;241m=\u001b[39m PositionEmbeddingSine(N_steps, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mposition_embedding \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearned\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'position_embedding'"
     ]
    }
   ],
   "source": [
    "# Load parts into a fresh DETR model (separate load cell)\n",
    "# Build model from local python files rather than torch.hub\n",
    "import importlib\n",
    "import importlib.util\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "# Try to import a builder from common local module locations.\n",
    "workspace_root = Path('.')  # adjust if your project root is elsewhere\n",
    "sys.path.insert(0, str(workspace_root))\n",
    "\n",
    "candidates = [\n",
    "    ('detr', 'build_detr'),\n",
    "    ('detr', 'build'),\n",
    "    ('models.detr', 'build_detr'),\n",
    "    ('models.detr', 'build'),\n",
    "    ('models', 'build_detr'),\n",
    "    ('models', 'build'),\n",
    "    ('Danial', 'build_detr'),\n",
    "    ('Danial', 'build'),\n",
    "    ('', 'build_detr'),\n",
    "    ('', 'build'),\n",
    "]\n",
    "\n",
    "builder = None\n",
    "builder_name = None\n",
    "for mod_name, fn_name in candidates:\n",
    "    try:\n",
    "        if mod_name:\n",
    "            mod = importlib.import_module(mod_name)\n",
    "        else:\n",
    "            # attempt top-level\n",
    "            mod = importlib.import_module(fn_name)\n",
    "        if hasattr(mod, fn_name):\n",
    "            builder = getattr(mod, fn_name)\n",
    "            builder_name = f\"{mod_name}.{fn_name}\" if mod_name else fn_name\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if builder is None:\n",
    "    # Try to locate common filenames in the workspace and import by path\n",
    "    found = False\n",
    "    for p in workspace_root.rglob('build*.py'):\n",
    "        spec = importlib.util.spec_from_file_location('local_build', str(p))\n",
    "        mod = importlib.util.module_from_spec(spec)\n",
    "        try:\n",
    "            spec.loader.exec_module(mod)\n",
    "            if hasattr(mod, 'build') or hasattr(mod, 'build_detr'):\n",
    "                builder = getattr(mod, 'build', getattr(mod, 'build_detr'))\n",
    "                builder_name = str(p)\n",
    "                found = True\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not found:\n",
    "        raise ImportError('Could not find a local DETR builder. Please provide a module with a `build` or `build_detr` function in the workspace.')\n",
    "\n",
    "print('Using builder:', builder_name)\n",
    "\n",
    "# The selected builder expects an `args` object with many attributes (see DETR's build signature).\n",
    "# We'll construct a minimal argparse.Namespace with sensible defaults. If `model2` exists (from earlier run), try to infer some values.\n",
    "# Typical DETR defaults (matching facebookresearch/detr):\n",
    "# - backbone: 'resnet50'\n",
    "# - device: 'cpu' or 'cuda'\n",
    "# - dataset_file: 'coco' (affects num_classes)\n",
    "# - hidden_dim: 256, nheads: 8, dim_feedforward: 2048, enc_layers: 6, dec_layers: 6\n",
    "# - num_queries: 100\n",
    "# - other training hyperparams used by build: lr_backbone, masks, dilation, aux_loss, etc.\n",
    "\n",
    "# Attempt to infer defaults from an existing model2 when available\n",
    "hidden_dim = 256\n",
    "nheads = 8\n",
    "enc_layers = 6\n",
    "dec_layers = 6\n",
    "num_queries = 100\n",
    "backbone_name = 'resnet50'\n",
    "\n",
    "if 'model2' in globals():\n",
    "    try:\n",
    "        if hasattr(model2, 'transformer') and hasattr(model2.transformer, 'd_model'):\n",
    "            hidden_dim = int(getattr(model2.transformer, 'd_model'))\n",
    "        if hasattr(model2.transformer, 'nhead'):\n",
    "            nheads = int(getattr(model2.transformer, 'nhead'))\n",
    "        # encoder/decoder layers counts are available on transformer implementation used here\n",
    "        if hasattr(model2.transformer, 'encoder') and hasattr(model2.transformer.encoder, 'num_layers'):\n",
    "            enc_layers = int(getattr(model2.transformer.encoder, 'num_layers'))\n",
    "        if hasattr(model2.transformer, 'decoder') and hasattr(model2.transformer.decoder, 'num_layers'):\n",
    "            dec_layers = int(getattr(model2.transformer.decoder, 'num_layers'))\n",
    "        if hasattr(model2, 'num_queries'):\n",
    "            num_queries = int(getattr(model2, 'num_queries'))\n",
    "        # backbone detection: fallback to resnet50\n",
    "        try:\n",
    "            backbone_name = getattr(model2.backbone.body, 'orig_name', backbone_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Build a namespace with the attributes the builder expects\n",
    "args = argparse.Namespace(\n",
    "    dataset_file='coco',\n",
    "    device='cpu',\n",
    "    backbone=backbone_name,\n",
    "    lr_backbone=0,\n",
    "    masks=False,\n",
    "    dilation=False,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout=0.1,\n",
    "    nheads=nheads,\n",
    "    dim_feedforward=2048,\n",
    "    enc_layers=enc_layers,\n",
    "    dec_layers=dec_layers,\n",
    "    pre_norm=False,\n",
    "    num_queries=num_queries,\n",
    "    aux_loss=False,\n",
    "    frozen_weights=None,\n",
    "    set_cost_class=1,\n",
    "    set_cost_bbox=5,\n",
    "    set_cost_giou=2,\n",
    "    bbox_loss_coef=5,\n",
    "    giou_loss_coef=2,\n",
    "    mask_loss_coef=1,\n",
    "    dice_loss_coef=1,\n",
    "    eos_coef=0.1,\n",
    ")\n",
    "\n",
    "# Build model using the args namespace\n",
    "try:\n",
    "    new_model = builder(args)\n",
    "except TypeError:\n",
    "    # Some builders return (model, ...) -> handle both cases\n",
    "    out = builder(args)\n",
    "    if isinstance(out, tuple):\n",
    "        new_model = out[0]\n",
    "    else:\n",
    "        new_model = out\n",
    "\n",
    "# If the builder returned (model, criterion, postprocessors) like in DETR, handle that\n",
    "if isinstance(new_model, tuple):\n",
    "    new_model = new_model[0]\n",
    "\n",
    "# Merge saved parts into new_model\n",
    "new_sd = new_model.state_dict()\n",
    "\n",
    "# Load backbone\n",
    "loaded_backbone = torch.load(paths['backbone'], map_location='cpu')\n",
    "for k, v in loaded_backbone.items():\n",
    "    new_sd['backbone.' + k] = v\n",
    "\n",
    "# Load transformer encoder\n",
    "loaded_te = torch.load(paths['transformer_encoder'], map_location='cpu')\n",
    "for k, v in loaded_te.items():\n",
    "    new_sd['transformer.encoder.' + k] = v\n",
    "\n",
    "# Load transformer decoder\n",
    "loaded_td = torch.load(paths['transformer_decoder'], map_location='cpu')\n",
    "for k, v in loaded_td.items():\n",
    "    new_sd['transformer.decoder.' + k] = v\n",
    "\n",
    "# Load head (merge keys as-is where appropriate)\n",
    "loaded_head = torch.load(paths['head'], map_location='cpu')\n",
    "for k, v in loaded_head.items():\n",
    "    # try exact key, otherwise attempt common prefixes\n",
    "    if k in new_sd:\n",
    "        new_sd[k] = v\n",
    "    elif 'class_embed.' + k in new_sd:\n",
    "        new_sd['class_embed.' + k] = v\n",
    "    else:\n",
    "        # best-effort: try to find a matching suffix\n",
    "        matches = [nk for nk in new_sd.keys() if nk.endswith(k)]\n",
    "        if matches:\n",
    "            new_sd[matches[0]] = v\n",
    "\n",
    "# Finally load into new_model with strict=False so missing/unexpected keys are allowed\n",
    "new_model.load_state_dict(new_sd, strict=False)\n",
    "print('Loaded parts into new_model (strict=False)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vengeance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
