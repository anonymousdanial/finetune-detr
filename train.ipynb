{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 18:50:31.528207: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ec4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Military objects in military environments/Dataset\"\n",
    "# ds = f\"military drones images\"\n",
    "\n",
    "models = f\"D:/23b6034/FYP/models/DETR-4\"\n",
    "train_dataset = f\"datasets/{ds}/train\"\n",
    "test_dataset = f\"datasets/{ds}/test\"\n",
    "val_dataset = f\"datasets/{ds}/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/d/23b6034/FYP/models/hub/facebookresearch_detr_main\n",
      "/mnt/d/23b6034/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/d/23b6034/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# laod the model\n",
    "model2 = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = model2.transformer\n",
    "print(\"Transformer model assigned to tf_model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Freeze the ResNet backbone\n",
    "# for param in model2.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Freeze the Transformer\n",
    "# for param in model2.transformer.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# print(\"ResNet and Transformer layers frozen.\")\n",
    "\n",
    "\"\"\" this block is optional, uncomment for faster training but worse results\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the architecture of the head layer (classification and box prediction heads)\n",
    "print(\"Classification head architecture:\")\n",
    "print(model2.class_embed)\n",
    "\n",
    "print(\"\\nBox prediction head architecture:\")\n",
    "print(model2.bbox_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, label_dir, classes, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt') # Assuming label files have the same name as images with a .txt extension\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    class_id, center_x, center_y, width, height = map(float, line.strip().split())\n",
    "                    # Convert YOLO format (center_x, center_y, width, height) to [x_min, y_min, x_max, y_max]\n",
    "                    # Assuming coordinates are normalized (0 to 1)\n",
    "                    x_min = center_x - width / 2\n",
    "                    y_min = center_y - height / 2\n",
    "                    x_max = center_x + width / 2\n",
    "                    y_max = center_y + height / 2\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(int(class_id)) # Assuming class_id is an integer\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0 : \"Tank\",\n",
    "    1 : \"drone\",\n",
    "    2 : \"people\",\n",
    "    3 : \"soldier\"\n",
    "}\n",
    "#tank drone people soldier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the classes dictionary\n",
    "\n",
    "\n",
    "# Define image and label directory paths\n",
    "train_image_dir = train_dataset + \"/images\"\n",
    "train_label_dir = train_dataset + \"/labels\"\n",
    "val_image_dir = val_dataset + \"/images\"\n",
    "val_label_dir = val_dataset + \"/labels\"\n",
    "test_image_dir = test_dataset + \"/images\"\n",
    "test_label_dir = test_dataset + \"/labels\"\n",
    "\n",
    "# Define a suitable transform\n",
    "# For DETR, a common transform resizes the image and normalizes it\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more transforms if needed, e.g., resizing to a fixed size expected by DETR\n",
    "    # transforms.Resize((800, 800)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate the DroneDataset for each set\n",
    "train_drone_dataset = DroneDataset(train_image_dir, train_label_dir, classes, transform=transform)\n",
    "val_drone_dataset = DroneDataset(val_image_dir, val_label_dir, classes, transform=transform)\n",
    "test_drone_dataset = DroneDataset(test_image_dir, test_label_dir, classes, transform=transform)\n",
    "\n",
    "\n",
    "# Instantiate DataLoader objects for each dataset\n",
    "batch_size = 1  # You can adjust this based on your GPU memory\n",
    "# Added num_workers for parallel data loading\n",
    "train_dataloader = DataLoader(train_drone_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), num_workers=0)\n",
    "val_dataloader = DataLoader(val_drone_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)), num_workers=0)\n",
    "test_dataloader = DataLoader(test_drone_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)), num_workers=0)\n",
    "\n",
    "print(\"DataLoaders for train, validation, and test datasets instantiated with num_workers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36606247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes from your defined classes dictionary\n",
    "num_classes = len(classes) # This will be 11 (0-10)\n",
    "\n",
    "# Modify the classification head (class_embed) to match the number of classes in your dataset\n",
    "# The input features should remain the same as the output features of the transformer decoder\n",
    "model2.class_embed = torch.nn.Linear(model2.class_embed.in_features, num_classes)\n",
    "\n",
    "# Move the modified model to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2.to(device)\n",
    "\n",
    "print(f\"Model's classification head modified to output {model2.class_embed.out_features} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c66458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    DETR Loss Function with Hungarian Matching\n",
    "\n",
    "    This loss computes the optimal bipartite matching between predicted and ground truth objects,\n",
    "    and then computes classification and bounding box regression losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, matcher_cost_class=1, matcher_cost_bbox=5,\n",
    "                 matcher_cost_giou=2, loss_ce=2, loss_bbox=2.5, loss_giou=2,\n",
    "                 eos_coef=0.1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_classes: number of object categories\n",
    "        - matcher_cost_class: relative weight of classification error in matching cost\n",
    "        - matcher_cost_bbox: relative weight of L1 error of bounding box coordinates in matching\n",
    "        - matcher_cost_giou: relative weight of giou loss of bounding box in matching\n",
    "        - loss_ce: relative weight of classification loss\n",
    "        - loss_bbox: relative weight of L1 bounding box loss\n",
    "        - loss_giou: relative weight of giou bounding box loss\n",
    "        - eos_coef: relative classification weight applied to the no-object category\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher_cost_class = matcher_cost_class\n",
    "        self.matcher_cost_bbox = matcher_cost_bbox\n",
    "        self.matcher_cost_giou = matcher_cost_giou\n",
    "        self.loss_ce = loss_ce\n",
    "        self.loss_bbox = loss_bbox\n",
    "        self.loss_giou = loss_giou\n",
    "        self.eos_coef = eos_coef\n",
    "\n",
    "        # Build weight vector for classification loss\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef  # Background class\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def hungarian_matching(self, outputs, targets):\n",
    "        batch_size, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # Flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes + 1]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Concatenate all target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost\n",
    "        cost_class = -out_prob[:, tgt_ids].log() # ------------------------------------------------------------ calculates the \"how wrong the model's classification is\" = measures confidence\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1) # ---------------------------------------------------- calcs the \"how wring the model's box location is compared to the ground truth\" - Compute negative log-likelihood classification cost (higher for wrong predictions) = uses simple l1 distance, absolute difference between coordinates\n",
    "\n",
    "\n",
    "        # Compute the GIoU cost between boxes\n",
    "        cost_giou = -self.generalized_box_iou(\n",
    "            self.box_cxcywh_to_xyxy(out_bbox),\n",
    "            self.box_cxcywh_to_xyxy(tgt_bbox)\n",
    "        ) # --------------------------------------------------------------------------------------------------- same as before, measures how wrong the box location is, but considering the spaces between the boxes - generalised intersction over union = gives sxore of -1 to +1\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = (self.matcher_cost_bbox * cost_bbox +\n",
    "             self.matcher_cost_class * cost_class +\n",
    "             self.matcher_cost_giou * cost_giou)\n",
    "        C = C.view(batch_size, num_queries, -1).cpu() # ------------------------------------------------------- combines all the three above costs into one single cost matrix\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = []\n",
    "\n",
    "        for i, c in enumerate(C.split(sizes, -1)):\n",
    "            # Hungarian algorithm on the detached CPU tensor\n",
    "            pred_indices, target_indices = linear_sum_assignment(c[i].detach().cpu().numpy()) # ---------------- looks at the entire cost matrix and finds the single best way to pair the predicted objects = weighted sum of three individual costs\n",
    "            indices.append((torch.as_tensor(pred_indices, dtype=torch.int64),\n",
    "                          torch.as_tensor(target_indices, dtype=torch.int64)))\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))\n",
    "                for i, j in indices]\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices):\n",
    "        \"\"\"Classification loss (Cross Entropy)\"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                   dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "        # Ensure target_classes is on the same device as src_logits\n",
    "        target_classes = target_classes.to(src_logits.device)\n",
    "\n",
    "        # Ensure empty_weight is on the same device as src_logits\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight.to(src_logits.device))\n",
    "        return loss_ce\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "        loss_bbox = loss_bbox.sum() / len(target_boxes) if len(target_boxes) > 0 else torch.tensor(0.0, device=src_boxes.device)\n",
    "\n",
    "        loss_giou = 1 - torch.diag(self.generalized_box_iou(\n",
    "            self.box_cxcywh_to_xyxy(src_boxes),\n",
    "            self.box_cxcywh_to_xyxy(target_boxes)))\n",
    "        loss_giou = loss_giou.sum() / len(target_boxes) if len(target_boxes) > 0 else torch.tensor(0.0, device=src_boxes.device)\n",
    "\n",
    "        return loss_bbox, loss_giou\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        \"\"\"Permute predictions following indices\"\"\"\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def box_cxcywh_to_xyxy(self, x):\n",
    "        \"\"\"Convert boxes from (cx, cy, w, h) to (x1, y1, x2, y2) format\"\"\"\n",
    "        x_c, y_c, w, h = x.unbind(-1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "\n",
    "    def generalized_box_iou(self, boxes1, boxes2): # -------------------------------------------------------------------------------------------------------- explain the intersection over union formula -\n",
    "        \"\"\"\n",
    "        Generalized IoU from https://giou.stanford.edu/\n",
    "        The boxes should be in [x0, y0, x1, y1] format\n",
    "        \"\"\"\n",
    "        # Ensure boxes are valid\n",
    "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "\n",
    "        # Compute intersection\n",
    "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "        # Compute union\n",
    "        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "        union = area1[:, None] + area2 - inter\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = inter / union\n",
    "\n",
    "        # Compute the area of the smallest enclosing box\n",
    "        lti = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "        rbi = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "        whi = (rbi - lti).clamp(min=0)  # [N,M,2]\n",
    "        areai = whi[:, :, 0] * whi[:, :, 1]\n",
    "\n",
    "        return iou - (areai - union) / areai\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        This performs the loss computation.\n",
    "\n",
    "        Args:\n",
    "            outputs: dict of tensors with keys:\n",
    "                - pred_logits: Tensor of dim [batch_size, num_queries, num_classes + 1]\n",
    "                - pred_boxes: Tensor of dim [batch_size, num_queries, 4] in cxcywh format\n",
    "            targets: list of dicts, such that len(targets) == batch_size.\n",
    "                Each dict should contain:\n",
    "                - labels: Tensor of dim [num_objects] containing the class labels\n",
    "                - boxes: Tensor of dim [num_objects, 4] containing the boxes in cx,cy,w,h format\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the losses\n",
    "        \"\"\"\n",
    "        # Retrieve the matching between the outputs of the model and the targets\n",
    "        indices = self.hungarian_matching(outputs, targets)\n",
    "\n",
    "        # Compute all the losses\n",
    "        loss_ce = self.loss_labels(outputs, targets, indices)\n",
    "        loss_bbox, loss_giou = self.loss_boxes(outputs, targets, indices)\n",
    "\n",
    "        # Combine losses\n",
    "        losses = {\n",
    "            'loss_ce': loss_ce * self.loss_ce,\n",
    "            'loss_bbox': loss_bbox * self.loss_bbox,\n",
    "            'loss_giou': loss_giou * self.loss_giou,\n",
    "        }\n",
    "\n",
    "        # Total loss\n",
    "        losses['loss_total'] = sum(losses.values())\n",
    "\n",
    "        return losses\n",
    "# criterion = DETRLoss(model2.class_embed.out_features - 1) # really just 11 output dims\n",
    "criterion = DETRLoss(\n",
    "    num_classes=model2.class_embed.out_features - 1, # 11\n",
    "    matcher_cost_class=1,\n",
    "    matcher_cost_bbox=5,\n",
    "    matcher_cost_giou=2,\n",
    "    loss_ce=1,\n",
    "    loss_bbox=5,\n",
    "    loss_giou=2,\n",
    "    eos_coef=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0680c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import torch.optim as optim # Import the optim module\n",
    "\n",
    "num_epochs = 300\n",
    "# Move the model to the device (GPU if available)\n",
    "model2.to(device)\n",
    "\n",
    "# Define a directory to save checkpoints\n",
    "checkpoint_dir = models\n",
    "os.makedirs(checkpoint_dir, exist_ok=True) # just make the dir if not there\n",
    "\n",
    "start_epoch = 0\n",
    "optimizer = None  # Initialize optimizer to None\n",
    "\n",
    "# Check for existing checkpoints to resume training\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "if checkpoints:\n",
    "    print(\"Found existing checkpoints. Resuming training.\")\n",
    "    # Find the latest checkpoint\n",
    "    latest_checkpoint = max([os.path.join(checkpoint_dir, f) for f in checkpoints], key=os.path.getctime) # ---- if folder has .pth file, use the one trained latest\n",
    "    print(f\"Loading checkpoint from: {latest_checkpoint}\")\n",
    "    model2.load_state_dict(torch.load(latest_checkpoint, map_location=torch.device('cpu'))) # ------------------ load the thing\n",
    "    start_epoch = int(latest_checkpoint.split('_')[-1].split('.')[0]) # ---------------------------------------- set the epoch to the last trained epoch\n",
    "    print(f\"Resuming from epoch {start_epoch + 1}\")\n",
    "\n",
    "    optimizer = optim.AdamW(model2.parameters(), lr=1e-5)\n",
    "\n",
    "else:\n",
    "    print(\"No existing checkpoints found. Starting training from scratch.\")\n",
    "    optimizer = optim.AdamW(model2.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b17729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#validation call\n",
    "\n",
    "def validate_model(model, dataloader, loss_fn, device, checkpoint_dir):\n",
    "    # Find the latest checkpoint\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found in the specified directory.\")\n",
    "        return None # Or raise an error\n",
    "\n",
    "    latest_checkpoint = max([os.path.join(checkpoint_dir, f) for f in checkpoints], key=os.path.getctime)\n",
    "    print(f\"Loading model from: {latest_checkpoint}\")\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for images, targets in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Use the correct loss function (DETRLoss)\n",
    "            loss = loss_fn(outputs, targets)['loss_total']\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Define the checkpoint directory\n",
    "checkpoint_dir = models # Use the same directory as in training\n",
    "\n",
    "# # Ensure the model is on the correct device before loading the state_dict\n",
    "# model2.to(device)\n",
    "\n",
    "# # Run validation\n",
    "# # Make sure to use the correct loss function (DETRLoss)\n",
    "# val_loss = validate_model(model2, val_dataloader, criterion, device, checkpoint_dir)\n",
    "# if val_loss is not None:\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging configuration\n",
    "log_file = os.path.join(checkpoint_dir, 'log.txt')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # This will still print to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Log training start information\n",
    "logging.info(\"=\"*50)\n",
    "logging.info(\"TRAINING SESSION STARTED\")\n",
    "logging.info(f\"Start epoch: {start_epoch}\")\n",
    "logging.info(f\"Total epochs: {num_epochs}\")\n",
    "logging.info(f\"Device: {device}\")\n",
    "logging.info(\"=\"*50)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs): # start from the latest epoch and end with the end epoch(300)\n",
    "    try: # -------------------------------------------------------------------------------------------------------------------------------------------------------------------- this is to handle errors\n",
    "        logging.info(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        model2.train() # ------------------------------------------------------------------------------------------------------------------------------------------------------ puts the model into training mode\n",
    "        running_loss = 0.0 # -------------------------------------------------------------------------------------------------------------------------------------------------- sets \"cumulative loss\" to 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Wrap the DataLoader with tqdm for a progress bar\n",
    "        for images, targets in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Move images and targets to the device (GPU if available)\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model2(images)\n",
    "\n",
    "\n",
    "            \n",
    "            losses = criterion(outputs, targets) # Calculate total loss using detrloss\n",
    "            loss = losses['loss_total']\n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "           #print(f\"the loss is {loss}\")\n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "           # print(f\"the losses is {losses}\") \n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "           # print(f\"the loss total is {losses['loss_total']}\") \n",
    "           # print(f\"with type {type(losses['loss_total'])}\")\n",
    "           # print(losses['loss_total'].requires_grad)\n",
    "\n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "           # print(\"---------------------------------------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "           # print(\"optimising done\")\n",
    "            loss.backward()\n",
    "           # print(\"backwards done\")\n",
    "            optimizer.step()\n",
    "           # print(\"stepping done\")\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Calculate average training loss for this epoch\n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        \n",
    "        # Log training results\n",
    "        logging.info(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Save the model checkpoint after each successful epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model2.state_dict(), checkpoint_path)\n",
    "        logging.info(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Run validation and log results\n",
    "        val_loss = validate_model(model2, val_dataloader, criterion, device, checkpoint_dir)\n",
    "        if val_loss is not None:\n",
    "            logging.info(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            logging.warning(f\"Epoch [{epoch+1}/{num_epochs}] - Validation returned None\")\n",
    "        \n",
    "        # Log epoch completion\n",
    "        logging.info(f\"Epoch {epoch+1} completed successfully\")\n",
    "        logging.info(\"-\" * 30)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"An error occurred during epoch {epoch+1}: {e}\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "        \n",
    "        logging.info(\"Attempting to load the last saved checkpoint and stopping training.\")\n",
    "        print(\"Attempting to load the last saved checkpoint and stopping training.\")\n",
    "        \n",
    "        # Find the last saved checkpoint before the error occurred\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "        if checkpoints:\n",
    "            last_checkpoint = max([os.path.join(checkpoint_dir, f) for f in checkpoints], key=os.path.getctime)\n",
    "            logging.info(f\"Loading checkpoint from: {last_checkpoint}\")\n",
    "            print(f\"Loading checkpoint from: {last_checkpoint}\")\n",
    "            try:\n",
    "                model2.load_state_dict(torch.load(last_checkpoint))\n",
    "                logging.info(\"Successfully loaded last checkpoint.\")\n",
    "                print(\"Successfully loaded last checkpoint.\")\n",
    "            except Exception as load_error:\n",
    "                error_msg = f\"Error loading checkpoint: {load_error}\"\n",
    "                logging.error(error_msg)\n",
    "                print(error_msg)\n",
    "        else:\n",
    "            logging.warning(\"No checkpoints found to load.\")\n",
    "            print(\"No checkpoints found to load.\")\n",
    "        \n",
    "        logging.info(\"Training stopped due to error.\")\n",
    "        break # Stop the training loop after encountering an error\n",
    "\n",
    "# Log training completion\n",
    "logging.info(\"=\"*50)\n",
    "logging.info(\"TRAINING SESSION ENDED\")\n",
    "logging.info(f\"Final epoch reached: {epoch+1}\")\n",
    "logging.info(\"=\"*50)\n",
    "\n",
    "# After training (or interruption), move the model back to CPU if needed for inference or saving\n",
    "# model2.to('cpu')\n",
    "# check epoch 27 and 74"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vengeance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
